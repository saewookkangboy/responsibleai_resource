"""
í†µí•© ì˜ˆì œ: 4ê°œ í”„ë¡œì íŠ¸ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ì™„ì „í•œ end-to-end ì˜ˆì œ
"""

import os
import sys
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "responsible_ai_automation"))
sys.path.insert(0, str(project_root / "ai-platform-validator"))

try:
    from main import ResponsibleAIAutomationSystem
    from src.validator import AIPlatformValidator
except ImportError:
    print("âš  ì¼ë¶€ ëª¨ë“ˆì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    ResponsibleAIAutomationSystem = None
    AIPlatformValidator = None


def step1_check_guidelines():
    """1ë‹¨ê³„: Guidelines ì²´í¬ë¦¬ìŠ¤íŠ¸ í™•ì¸"""
    print("\n" + "=" * 60)
    print("[1ë‹¨ê³„] Responsible AI Guidelines ì²´í¬ë¦¬ìŠ¤íŠ¸ í™•ì¸")
    print("=" * 60)
    
    checklist_path = project_root / "responsible-ai-guidelines" / "checklists" / "pre-project.md"
    
    if checklist_path.exists():
        print(f"âœ“ ì²´í¬ë¦¬ìŠ¤íŠ¸ íŒŒì¼ í™•ì¸: {checklist_path}")
        print("  â†’ í”„ë¡œì íŠ¸ ì‹œì‘ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ê²€í† í•˜ì„¸ìš”.")
    else:
        print(f"âš  ì²´í¬ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checklist_path}")
    
    return True


def step2_apply_policy():
    """2ë‹¨ê³„: Policy í…œí”Œë¦¿ ì ìš©"""
    print("\n" + "=" * 60)
    print("[2ë‹¨ê³„] Responsible AI Policy í…œí”Œë¦¿ ì ìš©")
    print("=" * 60)
    
    policy_path = project_root / "responsible-ai-policy" / "policies" / "api-service-policy.md"
    
    if policy_path.exists():
        print(f"âœ“ ì •ì±… í…œí”Œë¦¿ í™•ì¸: {policy_path}")
        print("  â†’ API ì„œë¹„ìŠ¤ ì •ì±… í…œí”Œë¦¿ì„ ì°¸ê³ í•˜ì—¬ ì •ì±…ì„ ìˆ˜ë¦½í•˜ì„¸ìš”.")
    else:
        print(f"âš  ì •ì±… í…œí”Œë¦¿ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {policy_path}")
    
    return True


def step3_validate_api():
    """3ë‹¨ê³„: AI Platform Validatorë¡œ API ê²€ì¦"""
    print("\n" + "=" * 60)
    print("[3ë‹¨ê³„] AI Platform Validatorë¡œ API ê²€ì¦")
    print("=" * 60)
    
    if AIPlatformValidator is None:
        print("âš  AI Platform Validatorë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    try:
        validator = AIPlatformValidator()
        print("âœ“ AI Platform Validator ì´ˆê¸°í™” ì™„ë£Œ")
        
        # API í‚¤ ê²€ì¦ (ì˜ˆì œ)
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            print("  â†’ API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            # ì‹¤ì œ ê²€ì¦ ìˆ˜í–‰
            # validation_result = validator.validate(api_key)
        else:
            print("  âš  API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        
        return True
    except Exception as e:
        print(f"âš  API ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False


def step4_evaluate_with_automation():
    """4ë‹¨ê³„: Responsible AI Automationìœ¼ë¡œ í‰ê°€"""
    print("\n" + "=" * 60)
    print("[4ë‹¨ê³„] Responsible AI Automationìœ¼ë¡œ í‰ê°€")
    print("=" * 60)
    
    if ResponsibleAIAutomationSystem is None:
        print("âš  Responsible AI Automationì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    try:
        # ì„¤ì • íŒŒì¼ ê²½ë¡œ
        config_path = project_root / "responsible_ai_automation" / "config.yaml"
        
        if not config_path.exists():
            print(f"âš  ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
            return False
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        system = ResponsibleAIAutomationSystem(str(config_path))
        print("âœ“ Responsible AI Automation ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        print("\n  â†’ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")
        X, y = make_classification(
            n_samples=1000,
            n_features=10,
            n_informative=5,
            n_redundant=2,
            n_classes=2,
            random_state=42
        )
        
        sensitive_features = pd.DataFrame({
            "gender": np.random.choice(["M", "F"], 1000),
            "race": np.random.choice(["A", "B", "C"], 1000),
        })
        
        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = train_test_split(
            X, y, sensitive_features, test_size=0.2, random_state=42
        )
        
        # ëª¨ë¸ í•™ìŠµ
        print("  â†’ ëª¨ë¸ í•™ìŠµ ì¤‘...")
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        print("  â†’ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        system.initialize_model(model, X_test, y_test, sensitive_test)
        
        # í‰ê°€ ìˆ˜í–‰
        print("  â†’ Responsible AI í‰ê°€ ìˆ˜í–‰ ì¤‘...")
        y_pred = model.predict(X_test)
        metrics = system.evaluate(X_test, y_test, y_pred, sensitive_test)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n  ğŸ“Š í‰ê°€ ê²°ê³¼:")
        print(f"    - ì „ì²´ Responsible AI ì ìˆ˜: {metrics.get('overall_responsible_ai_score', 0.0):.3f}")
        print(f"    - Responsible AI ê¸°ì¤€ ì¶©ì¡±: {'âœ“' if metrics.get('is_responsible', False) else 'âœ—'}")
        
        if "fairness" in metrics:
            print(f"    - ê³µì •ì„± ì ìˆ˜: {metrics['fairness'].get('overall_fairness_score', 0.0):.3f}")
        if "transparency" in metrics:
            print(f"    - íˆ¬ëª…ì„± ì ìˆ˜: {metrics['transparency'].get('overall_transparency_score', 0.0):.3f}")
        
        return True
        
    except Exception as e:
        print(f"âš  í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False


def step5_monitoring():
    """5ë‹¨ê³„: ì§€ì†ì  ëª¨ë‹ˆí„°ë§"""
    print("\n" + "=" * 60)
    print("[5ë‹¨ê³„] ì§€ì†ì  ëª¨ë‹ˆí„°ë§ ì„¤ì •")
    print("=" * 60)
    
    print("  â†’ Responsible AI Automation ëª¨ë‹ˆí„°ë§ì„ í™œì„±í™”í•˜ì„¸ìš”.")
    print("  â†’ python main.py --config config.yaml --mode monitor")
    print("  â†’ ëŒ€ì‹œë³´ë“œ: http://localhost:8080")
    
    return True


def main():
    """ë©”ì¸ í•¨ìˆ˜: ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
    print("=" * 60)
    print("Responsible AI í†µí•© ì›Œí¬í”Œë¡œìš° ì˜ˆì œ")
    print("=" * 60)
    
    steps = [
        ("Guidelines ì²´í¬ë¦¬ìŠ¤íŠ¸ í™•ì¸", step1_check_guidelines),
        ("Policy í…œí”Œë¦¿ ì ìš©", step2_apply_policy),
        ("API ê²€ì¦", step3_validate_api),
        ("Responsible AI í‰ê°€", step4_evaluate_with_automation),
        ("ëª¨ë‹ˆí„°ë§ ì„¤ì •", step5_monitoring),
    ]
    
    results = {}
    
    for step_name, step_func in steps:
        try:
            result = step_func()
            results[step_name] = result
        except Exception as e:
            print(f"\nâš  {step_name} ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            results[step_name] = False
    
    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ê²°ê³¼")
    print("=" * 60)
    
    for step_name, result in results.items():
        status = "âœ“ ì™„ë£Œ" if result else "âœ— ì‹¤íŒ¨"
        print(f"  {step_name}: {status}")
    
    success_count = sum(1 for r in results.values() if r)
    total_count = len(results)
    
    print(f"\n  ì´ {total_count}ê°œ ë‹¨ê³„ ì¤‘ {success_count}ê°œ ì™„ë£Œ")
    
    if success_count == total_count:
        print("\n  ğŸ‰ ëª¨ë“  ë‹¨ê³„ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\n  âš  ì¼ë¶€ ë‹¨ê³„ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()

